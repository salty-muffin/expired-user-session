{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            "name": "server opt-1.3b english",
            "type": "debugpy",
            "request": "launch",
            "program": "app.py",
            "cwd": "${workspaceFolder}/server",
            "env": {
                "HF_HOME": "${workspaceFolder}/server/models",
                "TORCH_HOME": "${workspaceFolder}/server/models",
            },
            "args": [
                "--whisper_model=openai/whisper-base.en",
                "--whisper_dtype=float16",
                "--gpt_model=facebook/opt-1.3b",
                "--gpt_temperature=1.1", // default: 1.0
                "--gpt_top_k=50", // default: 50
                "--gpt_top_p=1.0", // default: 1.0
                "--gpt_do_sample",
                "--bark_model=suno/bark",
                "--bark_semantic_temperature=1.0", // default: 0.7
                "--bark_coarse_temperature=0.6", // default: 0.7
                "--wtpsplit_model=segment-any-text/sat-3l-sm",
                "prompts.yml"
            ],
            "console": "integratedTerminal"
        },
        {
            "name": "server Llama-3.2-1B multilang",
            "type": "debugpy",
            "request": "launch",
            "program": "app.py",
            "cwd": "${workspaceFolder}/server",
            "env": {
                "HF_HOME": "${workspaceFolder}/server/models",
                "TORCH_HOME": "${workspaceFolder}/server/models",
            },
            "args": [
                "--whisper_model=openai/whisper-medium",
                "--whisper_dtype=float16",
                "--gpt_model=meta-llama/Llama-3.2-1B",
                "--gpt_temperature=1.1", // default: 1.0
                "--gpt_top_k=50", // default: 50
                "--gpt_top_p=1.0", // default: 1.0
                "--gpt_do_sample",
                "--bark_model=suno/bark",
                "--bark_semantic_temperature=1.0", // default: 0.7
                "--bark_coarse_temperature=0.6", // default: 0.7
                "--wtpsplit_model=segment-any-text/sat-3l-sm",
                "--languages=english,german",
                "--default_language=english",
                "prompts.yml"
            ],
            "console": "integratedTerminal"
        },
        {
            "name": "server Llama-3.2-8B multilang",
            "type": "debugpy",
            "request": "launch",
            "program": "app.py",
            "cwd": "${workspaceFolder}/server",
            "env": {
                "HF_HOME": "${workspaceFolder}/server/models",
                "TORCH_HOME": "${workspaceFolder}/server/models",
            },
            "args": [
                "--whisper_model=openai/whisper-medium",
                "--whisper_dtype=float16",
                "--gpt_model=meta-llama/Llama-3.1-8B",
                "--gpt_ctranslate_dir=models/ctranslate2/Llama-3.1-8B",
                "--gpt_dtype=int8_float16",
                "--gpt_temperature=1.1", // default: 1.0
                "--gpt_top_k=50", // default: 50
                "--gpt_top_p=1.0", // default: 1.0
                "--gpt_do_sample",
                "--bark_model=suno/bark",
                "--bark_semantic_temperature=1.0", // default: 0.7
                "--bark_coarse_temperature=0.6", // default: 0.7
                "--wtpsplit_model=segment-any-text/sat-3l-sm",
                "--languages=english,german",
                "--default_language=english",
                "prompts.yml"
            ],
            "console": "integratedTerminal"
        },
        {
            "name": "server opt-1.3b translate",
            "type": "debugpy",
            "request": "launch",
            "program": "app.py",
            "cwd": "${workspaceFolder}/server",
            "env": {
                "HF_HOME": "${workspaceFolder}/server/models",
                "TORCH_HOME": "${workspaceFolder}/server/models",
            },
            "args": [
                "--whisper_model=openai/whisper-medium",
                "--whisper_dtype=float16",
                "--gpt_model=facebook/opt-1.3b",
                // "--gpt_ctranslate_dir=models/ctranslate2/opt-1.3b",
                // "--gpt_activation_scales=models/activation_scales/opt-1.3b.pt",
                // "--gpt_dtype=int8_float16",
                "--gpt_dtype=bfloat16",
                "--gpt_temperature=1.1", // default: 1.0
                "--gpt_top_k=50", // default: 50
                "--gpt_top_p=1.0", // default: 1.0
                "--gpt_do_sample",
                "--bark_model=suno/bark",
                "--bark_semantic_temperature=1.0", // default: 0.7
                "--bark_coarse_temperature=0.6", // default: 0.7
                "--wtpsplit_model=segment-any-text/sat-3l-sm",
                "--languages=english,german",
                "--default_language=english",
                "--translate",
                "--opus_model_names_base=Helsinki-NLP/opus-mt-{}-{}",
                "--opus_ctranslate_dir=models/ctranslate2/opus-mt",
                "--opus_dtype=int8_float16",
                "prompts.yml"
            ],
            "console": "integratedTerminal"
        },
        {
            "name": "run text generation tests",
            "type": "debugpy",
            "request": "launch",
            "program": "tests/test_gpt.py",
            "args": [
                "--runs=5",
                "--iterations=10",
                "--prompt=Hello, is anybody out there?",
                "--language=english",
                "--model=meta-llama/Llama-3.1-8B",
                // "--ctranslate_dir=server/models/ctranslate2/opt-1.3b",
                // "--activation_scales=server/models/activation_scales/opt-1.3b.pt",
                // "--dtype=int8",
                // "--device=cpu",
                "--temperature=1.0",
                "--top_k=50",
                "--top_p=1.0",
                "--do_sample"
            ],
            "console": "integratedTerminal"
        }
    ]
}